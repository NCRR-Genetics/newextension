# Data management and wrangling {#wrangling}

![](https://img.shields.io/badge/document%20status-in%20progress-orange?style=flat-square)

> When in RStudio, quickly jump to this page using `r3::open_data_wrangling()`.

**Session objectives**:

1. Learn the difference between "messy" and "tidy" data 
and how to create tidy data to simplify your analysis.
1. Perform simple transformations and subsetting of datasets, such as:
    - Subset specific columns and rows of a dataset (with `filter()`and `select()`).
    - Sort rows of a dataset by a specific column (with `arrange()`).
    - Create new or transform existing columns in a dataset (with `mutate()`).
    - Calculate simple data summaries (with `summarize()`).
1. Learn about and apply the "split-apply-combine" method for doing analyses
(with `group_by()` and `summarize()`).
1. Know the difference between "long" and "wide" data 
and how to convert between them by "pivotting" (with `pivot_longer()`
and `pivot_wider()`).
1. Write "tidier" and more readable code by using the pipe (`%>%`) operator.

## "Messy" vs. "tidy" data

Before we begin this section, 
**take 5 min to read through this *"Messy" vs "tidy" data* section**.
Afterward, we'll go through it together and discuss it.

The concept of "tidy" data was popularized in an [article] by Hadley Wickham
and described in more detail in the [Tidy Data chapter] of the R for Data Science online
book. A tidy dataset is when:

- Each variable has its own column (e.g. "Body Weight")
- Each observation has its own row (e.g. "Person")
- Each value has its own cell (e.g. "Body weight for a person at a specific date")

[article]: http://www.jstatsoft.org/v59/i10/paper.
[Tidy Data chapter]: https://r4ds.had.co.nz/tidy-data.html

Take a look at the example "tidy" and "messy" data frames
(also called "tibbles" in the tidyverse) below.
Think about why each is "tidy" or "messy". 
What do you notice between the tidy versions and the messier versions?

```{r}
# Datasets come from tidyr
# Tidy:
table1
# Partly tidy:
table2
# Messier:
table3
# Messy:
table4a
# Messy:
table4b
```

The "most" tidy version is `table1` which has columns that describe their values
(e.g. population is population size), each row is unique (e.g. first row is for 
values from Afghanistan from 1999), and each cell is an explicit value
representative of its column and row. 
`table2` is a "long" version of `table1` so it is partly "tidy", 
but it doesn't satisfy the rule that each variable has a column,
since `count` represents both cases and population size.
On the other hand, `table3` is messy
because the `rate` column values are a composite of two other column values
(cases and population), when it should be a single number (a percent). Both
`table4a` and `table4b` have columns with ambiguous values inside; what does
values in the `1999` column contain? You can't tell from the data.

Tidy data has a few notable benefits:

1. Time spent preparing your data to be tidy from the beginning can save days
of frustration in the long run.
2. "Tidy data" is a conceptual framework that allows you to easy build off and
wrangle (i.e. "manipulate", "clean up", "manage") data in simpler 
and easy to interpret ways, especially when using the tidyverse packages 
(which are an ecosystem of related packages that share similar designs and work
well together).

The concept of tidy data also gives rise to the concept of tidy code. By using
"verbs" (R functions), chaining them together in "sentences" (in a sequential
pipeline), you can construct meaningful and readable code that, in more plain
English, describes what you are doing to the data.

## Managing and working with data in R

**Take 5 min and read through this section**, then we'll go through it together.
When working with data, there are a few principles to follow:

- **Never** edit raw data and save it in a separate location 
(could put in the `data-raw/` folder)
    - *Note*: Saving to `data-raw/` depends on how you collected the data 
    and how many collaborators are on your team. 
    You may end up storing and processing the data in another folder as a project
    of its own.
- Only work with your raw data using R code, *don't manually edit it*.
Manual editing doesn't leave a history of what you've done to it,
so you can't go back and see what you've done. 
Always keep a history of any changes you've made to the data,
preferably by using R code.
- Save the edited data as another dataset and store it in the `data/` folder.

When wrangling your data with R code make sure to:

- Document and comment as best you can what you did to your data and why you did
it to help you remember
- Write the code itself to be as descriptive as you can
and to readable enough to understand what is being done to the data.
Keep the code simple: Don't be clever, be clear.
Clear code is easier to understand than some clever code.

In data wrangling, 
most tasks can be expressed by a few simple "verbs" (actions).
Wrangling here is used in the sense of maneuvering, managing, controlling, and
turning your data around to clean it up, to better understand it,
and to prepare it for later analyses. 
The table below lists some common "verbs":

```{r table-wrangling-verbs, echo=FALSE}
tibble::tribble(
    ~Task, ~Example, ~Function,
    "Select columns", "Remove data entry columns such as person's name who entered the data.", "`select()`",
    "Rename columns", "Changing a column name from 'Q1' to 'ParticipantName'.", "`rename()`",
    "Transform or modify columns", "Multiplying a column's values or taking the log.", "`mutate()`",
    "Subset/filter rows", "Keeping rows with glucose values above 4.", "`filter()`",
    "Sort rows", "Show rows with the smallest value at the top.", "`arrange()`",
    "Convert data from wide to long", "One row per participant to multiple participants per row (repeated measures).", "`pivot_longer()`",
    "Convert data from long to wide", "Multiple rows per participant (repeated measures) to one participant per row.", "`pivot_wider()`",
    "Calculate summaries of the data", "Calculating the maximum, median, and minimum age.", "`summarise()`",
    "Run an analysis by a group", "Calculate means of age by males and females.", "`group_by()` with `summarise()`"
) %>% 
    knitr::kable(caption = "List of common data wrangling tasks, along with an example and the function used for the wrangling.")
```

The functions above come from the packages [dplyr] and [tidyr].
Thes packages provide easy tools for most common data manipulation tasks. 
For dplyr, it is built to work directly with data frames 
(i.e. rectangular data like those found in spreadsheets)
and has an additional feature to interact directly with data stored in an external
database, such as in SQL. Working with databases is a powerful way to work
with massive datasets (100s of GB), more than what your computer could normally
handle. Working with massive data won't be covered in this course,
but see this [resource from Data Carpentry](https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html))
to learn more.

[dplyr]: https://dplyr.tidyverse.org/
[tidyr]: https://tidyr.tidyverse.org/

**Tip**: Sometimes you need to do some complicated wrangling to get your data 
in appropriate "shape" to use for later analyses.
To help save some time,
you could save the wrangled data as an "output" dataset in the `data/` folder.
That way, you can easily use it again later rather 
than having to run the wrangling code every time you want to work with the data.

## Load the packages and dataset

We're going to use the US [NHANES] dataset to demonstrate the wrangling functions. 
There is an [NHANES package] that contains a teaching version of the original dataset, 
so we'll use that for this lesson. 
First, make sure the R Project you created previously is open. Then open
the `R/wrangling-session.R` script to start typing out the next code. We'll use
this file to write the code for this session (but not for the exercises).

[NHANES]: https://www.cdc.gov/nchs/nhanes/index.htm
[NHANES package]: https://CRAN.R-project.org/package=NHANES

```{r, message=FALSE, warning=FALSE}
# Load up the packages
# tidyverse contains the dplyr and tidyr packages
library(tidyverse)
library(NHANES)

# Briefly glimpse contents of dataset
glimpse(NHANES)
```

## Exercise: Become familiar with the dataset

Time: 5 min

Take the time to get familiar with the NHANES dataset.

1. Create a new R script by typing in the RStudio Console
`usethis::use_r("exercises-wrangling")`. 
1. Copy the code below and paste into the new exercise file.
1. Replace the `___` with the `NHANES` dataset. 
1. Run each line of code by typing `Ctrl-Enter`.

```{r exercise-familiar-with-data, eval=FALSE}
# Load the packages
library(tidyverse)
library(NHANES)

# Check column names
colnames(___)

# Look at contents
str(___)
glimpse(___)

# See summary
summary(___)

# Look over the dataset documentation
?___
```

<details><summary><strong>Click for the solution</strong></summary>
<p>

```{r, eval=FALSE}
# load the packages
library(tidyverse)
library(NHANES)

# Check column names
colnames(NHANES)

# Look at contents
str(NHANES)
glimpse(NHANES)

# See summary
summary(NHANES)

# Look over the dataset documentation
?NHANES
```

</p>
</details>

## Select specific columns in a dataset

Selecting columns of a dataset is a very common data wrangling task.
The function for this task is appropriately called `select()`.
For the input arguments, it takes the dataset as the first argument 
(first position after the `(`)
and then takes the names of the columns you want to select.
Because the argument after the data argument is `...`, 
it means that you can add as many columns as you want, separated by a `,`.

```{r}
# Select one column by its name, without quotes
select(NHANES, Age)

# Select two or more columns by name, without quotes
select(NHANES, Age, Weight, BMI)

# To *exclude* a column, use minus (-)
select(NHANES, -HeadCirc)
```


If some of your columns have similar patterns at the beginning, within, or end,
you can use the helper functions to choose these columns.
Use `?select_helpers` (choose the first option in the menu that pops up)
to read more about these functions and to get help on them. 
Some more useful helpers are:

- `starts_with()`: Select columns that begin with a pattern.
- `ends_with()`: Select columns that end with a pattern.
- `contains()`: Select columns that contain a pattern.

```{r}
# All columns starting with letters "BP" (blood pressure)
select(NHANES, starts_with("BP"))
# All columns ending in letters "Day"
select(NHANES, ends_with("Day"))
# All columns containing letters "Age"
select(NHANES, contains("Age"))
```


For more information on using the pattern functions such as `starts_with()`,
check `?select_helpers`. 

You'll notice that running these functions doesn't actually change the data itself.
When you run a function without assigning it using `<-`,
the only action the function does is to send the output to your screen.
But if you want to create a new dataset with only the columns you selected,
you'll need to assign it to a new object.

```{r}
# Recall the style guide for naming objects
nhanes_blood_pressure <- select(NHANES, starts_with("BP"))
nhanes_blood_pressure
```

## Rename specific columns

Depending on how your data was collected, 
it may have column names that aren't very descriptive.
So you'll probably want to rename them to something more explanatory.
As with `select()`, the rename you use the function called `rename()`.
Like `select()`, `rename()` takes the dataset as the first argument
(first position) and then takes as many renaming arguments as you want
(because the second argument position is `...`).
Renaming takes the form of `newname = oldname`.

```{r}
rename(NHANES, NumberBabies = nBabies)
```

You can't really see what was changed. Let's make a new object of only `nBabies`.

```{r}
nhanes_number_babies <- select(NHANES, nBabies)
rename(nhanes_number_babies, NumberBabies = nBabies)
```

You can now see how it changes the name.
What if you want to select some columns and then rename some of them,
do you have to create a new data object every time? No! 
We can make use of a very powerful tool called piping (with the `%>%` function).

## `%>%`: The pipe operator

**Take 5 minutes and read this section** before we go over it together.

A key component of the tidy data and tidy code concept is making use of the `%>%` operator.
This operator allows you to "pipe" the output from one function to the
input of another function, like a plumbing pipe would do for water.
This allows you to easily chain functions together into "sentences".
Let's use an example based on English words for some action. 
This is the English sentence:

> We need some eggs. Drive to the grocery store
and buy up some eggs before coming home from work.

There are basically two actions here ("drive" and "buy")
and four "inputs" ("work", "grocery store", "eggs", "home"),
that are all based on the previous action.
Since an action in R is a function, 
the functions would be `drive()` and `buy()`.
In regular R, if we wanted to chain these functions together, 
we would have to nest them like this:

```{r, eval=FALSE}
drive(buy(drive(at_work, "grocery store"), "eggs"), "home")
```

This is difficult to read. We could also create temporary objects:

```{r, eval=FALSE}
at_grocery_store <- drive(at_work, "grocery store")
got_eggs <- buy(at_grocery_store, "eggs")
at_home <- drive(got_eggs, "home")
```

But this still isn't too "readable". The pipe `%>%` operator can really simplify this:

```{r, eval=FALSE}
at_work %>% 
    drive("grocery store") %>% 
    buy("eggs") %>% 
    drive("home")
```

Do you find this more readable and understandable?
We read it like how it would actually be done, in order of the steps taken.

Instead of nesting functions (reading from the inside to the outside), 
the idea of piping is to read the functions from left to right.
This can help clarify and break down complex data processing workflows,
and is the basis for all tidyverse and many other packages.
This is a basic design philosophy of interacting with data when using the tidyverse.

The pipe `%>%` takes the output from the object or function on the left hand side
and puts it into the function of the right hand side.
All input goes into the first position argument of the function.
So with tidyverse packages,
the first position always takes the data,
to make the function usable with the pipe.

**Ok, let's return back together and try this out**.

```{r}
# These two ways are the same
colnames(NAMES)
NHANES %>% 
    colnames()
```

Because the pipe automatically takes NHANES
and puts it into the first position, 
we don't need to type out `NHANES` inside `colnames()` when piping.

Let's try the pipe on the `select()` and `rename()` function from the previous section.
Remember, both `select()` and `rename()` take a dataset as the first position,
which makes them pipe-able.

```{r}
NHANES %>% 
    select(nBabies) %>% 
    rename(NumberBabies = nBabies)
```

We can now "read" these actions as:

> Take the NHANES dataset *and then* select the nBabies column 
*and then* rename the nBabies column to NumberBabies.

## Exercise: Practice what we've learned

Time: 8 min

In the `exercise-wrangling.R` file, complete these tasks:

1. Copy and paste the below code into the exercise file.
In the `select()` function, 
type in the columns `HomeOwn`, `TVHrsDay`, and `Diabetes`
where the blank space is.

    ```r
    NHANES %>% 
        select(___)
    ```
    
1. Copy and paste the below code and fill out the blanks.
Rename `DiabetesAge` to be `DiabetesAgeOfDiagnosis` and `Gender` to be `Sex` 
(gender is the social construct, while sex is biological).

    ```r
    NHANES %>% 
        rename(___ = ___, ___ = ____)
    ```

1. Re-write this bit of code to use the pipe:

    ```r
    select(NHANES, BMI, contains("Age"))
    ```

1. Read aloud (under your breath or in your head) the below code.
How intuitive is it to read?
Now re-write this code so you don't need to create the temporary `drug_use` object
by using the pipe,
then re-read the revised version. Which do you feel is easier to "read"?

    ```r
    drug_use <- select(NHANES, Marijuana, AgeFirstMarij)
    rename(drug_use, AgeOfFirstMarijuanaUse = AgeFirstMarij)
    ```

## Filter the data by row

Filtering data by row is a very common activity in data analysis, 
for example, to get rid of outliers or to subset by a categorical group.
As with the previous functions, the function to subset/filter is called `filter()`.
The `filter()` function takes a logic condition (`TRUE` or `FALSE`).
As with the other functions, the first position argument is the dataset,
and all others are logic conditions to use.
With `filter()`, when the logic conditions equals `TRUE`,
that means it **keeps** the rows that equal `TRUE` 
and *drop* those that are `FALSE`.

*A warning*: Since `filter()` uses logical conditions,
you need to be really careful when writing the logic.
As you probably know, humans are really really bad at logic.
So if your logical condition starts getting even a little complex,
double and triple check that you know your logic code is doing what you think it is.
It's very easy to make mistakes at this stage, even for advanced R users.

The simplest kind of logic condition is to test for "equality". 
In R, "equal to" is represented by `==`.
For example, if we want to keep only females in the dataset it would be:

```{r}
NHANES %>%
    filter(Gender == "female")
```

We'd "read" this code as:

> Take the NHANES dataset, *and then* 
filter so that only rows where `Gender` is equal to "female" are kept.

So, when a row in the `Gender` column has the value "female", that row is kept.
Otherwise, it is dropped.
There are other logic comparisons to use.
Use Table below as a reference for logical conditions in R.

TODO: Check about referencing tables.

```{r logic-operators, echo=FALSE}
tibble::tribble(
    ~Operator, ~ Description,
    "<", "less than",
    "<=", "less than or equal to",
    ">", "greater than",
    ">=", "greater than or equal to",
    "==", "equal to",
    "!=", "not equal to",
    "!x", "Not x (if x is true or false)",
    "x | y", "x OR y",
    "x & y", "x AND y"
) %>% 
    kable(caption = "Logical operators in R.")
```

Let's try out a few of these logic conditions with `filter()`.

```{r}
# When rows *don't* have female
NHANES %>%
    filter(Gender != "female")

# when BMI is equal to 25
NHANES %>%
    filter(BMI == 25)

# when BMI is equal to or more than 25
NHANES %>%
    filter(BMI >= 25)
```

We use the `|` ("or") and `&` ("and") when we want to combine conditions across columns.
Be especially careful with these operators and whenever combining logic conditions,
as they can sometimes work differently than our human brains interpret them
(speaking from experience).
For `&`, both sides must be `TRUE` in order for the combination to be `TRUE`.
For `|`, only one side needs to be `TRUE` in order for the combination to be `TRUE`.
To see how they work try these:

```{r}
TRUE & TRUE
TRUE & FALSE
FALSE & FALSE
TRUE | TRUE
TRUE | FALSE
FALSE | FALSE
```


```{r}
# when BMI is 25 *and* Gender is female
NHANES %>%
    filter(BMI == 25 & Gender == "female")

# when BMI is 25 *or* gender is female
NHANES %>%
    filter(BMI == 25 | Gender == "female")
```

## (Re)Arranging the rows of your data by column

You may want to sort your rows by a specific column so that rows
are arranged so that bigger (or smaller) values are at the top.
Arranging is done by using `arrange()`.
As again, `arrange()` takes the dataset as the first argument
and anything else it uses as the columns to order by.
By default, arrange orders in ascending order.

```{r}
# ascending order by age
NHANES %>%
    select(Age) %>% 
    arrange(Age)
```

We're selecting age first so we can show what is happening. 
It also arranges characters alphabetically.

```{r}
NHANES %>% 
    select(HealthGen) %>% 
    arrange(HealthGen)
```

We can do this also in descending order with `desc()`.

```{r}
# descending order
NHANES %>%
    select(Age) %>% 
    arrange(desc(Age))
```

You can also arrange by multiple columns. For instance,
first arrange by `Gender` and then by `Age`.

```{r}
# ascending order by Gender and Age
NHANES %>%
    select(Gender, Age) %>% 
    arrange(Gender, Age)
```

## Transform or add columns

To "transform" (modify) an existing column or to add a new one,
the function to use is called `mutate()`.
Unfortunately, unlike the other functions, 
the name is not as obvious about what it does.
The meaning of mutate though is to change or modify, so it kind of makes sense.
Like the other functions, the first input is the data 
and the other arguments are columns to add or modify.

The form that `mutate()` uses is similar to normal R assignment:
For instance, since the height's values are in centimeters, 
maybe we'd rather want them in meters. So, in `mutate()` we'd type out:

```
Height = Height / 100
```

This form is similar to how math works. 
The action that happens on the right hand side 
is put into the variable of the left hand side.
With using `mutate()` itself:

```{r}
NHANES %>%
    mutate(Height = Height / 100)
```

Or we can create a new column (maybe log transforming height):

```{r}
NHANES %>% 
    mutate(LoggedHeight = log(Height))
```

We can also add multiple modifications 
or additions with mutate by separating with `,`.
So if we first wanted to have height as meters and then take the log, 
it would be:

```{r}
NHANES %>% 
    mutate(Height = Height / 100,
           LoggedHeight = log(Height))
```

We can also have different values based on a logic conditions using `if_else()`.
Use Table to help with creating the logic condition.

TODO: Add table ref.

```{r}
NHANES %>%
    mutate(HighlyActive = if_else(PhysActiveDays >= 5, "yes", "no"))
```

Recall that the original dataset doesn't change. 
If we want the added variable to be included we must assign it to something with `<-`.
So putting it all together:

```{r}
NHANES_update <- NHANES %>%
    mutate(Height = Height / 100,
           LoggedHeight = log(Height),
           HighlyActive = if_else(PhysActiveDays >= 5, "Yes", "No"))
```

## Exercise: Piping, filtering, and mutating

Time: 15 min

Copy and paste the code below into the script `exercises-wrangling.R`. 
Then start replacing the `___` with the appropriate
code to complete the tasks below.
(*Suggestion*: Create a new "Section"
in the R script for this exercise by using `Ctrl-Shift-R`).

1. Filter NHANES so only those with BMI more than or equalt to 20 
*and* less than or equal to 40 *and* keep those who have diabetes.
1. Create a new variable called `UrineVolAverage` by calculating the average
urine volumne (from `UrineVol1` and `UrineVol2`).
    - *Comment*: After creating the `UrineVolAverage` column, 
    check out values. What do you nothice? Compare with the original urine columns.
1. Create a new variable called `YoungChild` when age is less than 6 years.

```{r, eval=FALSE}
# 1. BMI between 20 and 40 and who have diabetes
NHANES %>%
    # format: variable >= number or character
    filter(___ >= ___ & ___ <= ___ & ___ == ___)

# Pipe the data into mutate function and:
NHANES_modified <- ___ %>% # dataset
    mutate(
        # 2. Calculate average urine volume
        ___ = ___,
        # 3. Create YoungChild variable using a condition
        ___ = if_else(___, "Yes", "No")
    )
NHANES_modified
```

<details><summary><strong>Click for a possible solution</strong></summary>
<p>

```{r}
# 1. BMI between 20 and 40 and who have diabetes
NHANES %>%
    # format: variable >= number
    filter(BMI >= 20 & BMI <= 40 & Diabetes == "Yes")

# Pipe the data into mutate function and:
NHANES_modified <- NHANES %>% # dataset
    mutate(
        # 2. Calculate average urine volume
        UrineVolAverage = (UrineVol1 + UrineVol2) / 2,
        # 3. Create YoungChild variable using a condition
        YoungChild = if_else(Age < 6, "Yes", "No")
    )
```

For the "UrineVolAverage" values, they are probably almost entirely `NA` (aka
missing). That's because `NA` values are infectious. See how "UrineVol2" has mostly
`NA` values too? When you calculate something that has `NA`, you get another `NA`.
This is something to be careful about. So always check your calculations!

</p>
</details>

## `group_by()`, `summarise()`: Create a summary of the data, alone or by a group(s)

Using `group_by()` on its own does nothing. But, combine it with other functions
like `summarise()` and you can do some very powerful data processing. This
particular combination of commands allows for the [split-apply-combine]
technique to be easily used. Many data analysis tasks can be approached using
this paradigm: split the data into groups, apply some analysis to each group,
and then combine the results together. Using `group_by()` splits the data up and
`summarise()` then applies an analysis and then combines it back together. The
arguments to `group_by()` are the column names that contain the **categorical**
variables for what you want to calculate the summary statistics.

[split-apply-combine]: https://www.jstatsoft.org/article/view/v040i01

Let's first explore using `summarise()` on it's own, which outputs a single value
(e.g. a max or mean). Like `mutate()`, you can add multiple "summaries" by adding
new variables separated by commas.

```{r}
NHANES %>%
    summarise(MaxAge = max(Age, na.rm = TRUE),
              MeanBMI = mean(BMI, na.rm = TRUE))
```

Combine it with `group_by()`.

```{r}
# Grouped by gender
NHANES %>%
    group_by(Gender) %>% 
    summarise(MaxAge = max(Age, na.rm = TRUE),
              MeanBMI = mean(BMI, na.rm = TRUE))

# Grouped by gender and diabetes
NHANES %>%
    group_by(Gender, Diabetes) %>% 
    summarise(MeanAge = mean(Age, na.rm = TRUE),
              MeanBMI = mean(BMI, na.rm = TRUE))
```

## Convert data from wide to long form

The tidyr package is a companion package to dplyr and allows for a wide range of
manipulations of the data structure data. For example, sometimes we have data
in the wide form as it is often easier to enter data in wide form. Wide form can
be useful when presenting as tables. But there are usually problems with wide
form data, especially when it comes to analysing it. Look at the table below,
which is in wide form. Wide form data also tends to be a bit "messier" then long
form. For instance, what do the values in the `1999` column mean? Unless you had
a data dictionary, you would not know what those values mean.

```{r table-example-wide-data, echo=FALSE}
kable(table4b, caption = "Example wide form dataset.")
```

The long form on the other hand is usually better suited for analyses and
visualizing, especially when doing [split-apply-combine] techniques. Long form
data also tends to be more *tidy* compared to wide form.

```{r table-example-long-data, echo=FALSE}
kable(table1 %>% select(-cases), caption = "Example long form dataset.")
```

We can get to this from the wide form through wrangling the data like so:

(ref:wide-long) Convert from wide to long. Taken from [R for Data Science](https://r4ds.had.co.nz/tidy-data.html).

```{r image-wide-long, echo=FALSE, fig.cap="(ref:wide-long)"}
# TODO: fix this. Get image directly.
# include_graphics("https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png")
```

To go from wide to long we use `gather` to sweep up a set of columns into one
key-value pair. The **arguments** for `gather` are:

1. The name of a *new* column that contains the original column names
2. The name of a *new* column that contains the values from the original columns
3. The original columns we either want or do not want "gathered" up.

Compare how `table4b` looks normally and then after converting to the long form
with `gather()`.

```{r}
# Original data
table4b

# Convert to long form by stacking population by each year
# Use minue to exclude a variable (country) from being "gathered"
table4b %>% 
    gather(year, population, -country)

# This does the same:
table4b %>%
    gather(year, population, `1999`, `2000`)
```

This can be challenging to conceptually grasp at first. Let's try an example
situation when you may use `gather()`. Let's say you wanted to find out the
means of several characteristics of a population. You could use the `group_by()`
and `summarise()` combination on their own, but another method is using `gather()`
to group by and summarise all the variables at once.

```{r}
# Keep only variables of interest
nhanes_chars <- NHANES %>% 
    select(SurveyYr, Gender, Age, Weight, Height, BMI, BPSysAve)
nhanes_chars

# Convert to long form, excluding year and gender
nhanes_long <- nhanes_chars %>% 
    gather(Measure, Value, -SurveyYr, -Gender)
nhanes_long

# Calculate mean on each measure, by gender and year
nhanes_long %>% 
    group_by(SurveyYr, Gender, Measure) %>% 
    summarise(MeanValue = mean(Value, na.rm = TRUE))
```

Note that summarising by mean can only be done with continuous variables.
Anyway, with the long form, you can do more with less code `r emo::ji("relaxed")`

```{r nhanes-data}
NHANES
```

To look like this:

```{r nhanes-output, echo=FALSE}
NHANES %>% 
    mutate(MoreThan5DaysActive = if_else(PhysActiveDays >= 5, TRUE, FALSE)) %>% 
    select(SurveyYr, Gender, Age, Poverty, BMI, BPSysAve, BPDiaAve, TotChol,
           DiabetesAge, nBabies, MoreThan5DaysActive, AlcoholDay) %>% 
    rename(TotalCholesterol = TotChol, 
           NumberOfBabies = nBabies, 
           DrinksOfAlcoholInDay = AlcoholDay, 
           AgeDiabetesDiagnosis = DiabetesAge) %>% 
    filter(Age >= 18, Age <= 75) %>% 
    gather(Measure, Value, -SurveyYr, -Gender) %>% 
    group_by(SurveyYr, Gender, Measure) %>% 
    summarise(Mean = round(mean(Value, na.rm = TRUE), 2)) %>% 
    arrange(Measure, Gender, SurveyYr) %>% 
    spread(SurveyYr, Mean) %>% 
    knitr::kable(caption = "Mean values of characteristics for women and men between 18-75 years of age at each of the two survey years.")
```

## Reshape a dataframe from a wide to a long format using `pivot_longer()`

We can also do the opposite: a shift from a dataframe in wide format to a dataframe in long format. 

In this case, we want to specify:

- the argument `names_to`: the name of the new variable under which we want to group our columns, 
- the argument `values_to`: the name of the new variable under which we want to group the values of our variables.

We run this function...

```
population_wide %>%
  pivot_longer(names_to = "Country", values_to = "Population")
```

... but we get an error message!

This is because we did not specify which columns to shift into a long format.

  > In `pivot_wider()`, if no argument is given to `id_cols` it defaults to select all columns. However, `pivot_longer` has no default for `cols`, and will throw an error if no argument is given.
We could select every column, with the [tidyselect](https://r4ds.had.co.nz/transform.html#select) helper function `everything()`. This function tells `pivot_longer()` to use all the columns in the dataset when converting to long form.

```
population_wide %>%
  pivot_longer(everything(), names_to = "Country", values_to = "Population")
```

This breaks the row-wise relation between year and population, and is clearly not what we want. Instead, we specify that the variable `year` is not to be involved in the reshaping. Columns not involved in the reshape will be copied, to preserve the row-wise relations in the data. 

```
population_wide %>%
  pivot_longer(-year, names_to = "country_name", values_to = "population_data")
```

We use the `-` here for `year` to tell `pivot_longer()` to *not* include (to *exclude*) the column `year` from being converted to long form.

**Notice that** in the function `pivot_longer()` the arguments `names_to` and `values_to` require "" because we are providing names of variables that need to be created. 
Instead, in the function `pivot_wider()` we are giving the arguments `names_from` and `values_from` variables that already exist, hence  the "" are not needed.


### Wide and long formats

The illustrations below shows how the same information can be represented in a long and a wide format. Notice that the value (a-f) of each variable/key (x,y,z) can be read for each id (1, 2) in both formats.

![*Img: https://github.com/gadenbuie/tidyexplain*](https://github.com/gadenbuie/tidyexplain/raw/master/images/static/png/original-dfs-tidy.png){width=60%}

The functions we want to use for these operations are `pivot_longer()` and `pivot_wider()` from the tidyr package.

## Reshape a dataframe from a long to a wide format with `pivot_wider()`

You can also convert to wide from long. This is not as commonly done, since most
analyses are better in the long form. But sometimes you may need to have a wide
form. Here you can use `spread()`, which takes three arguments: 1) the data, 2)
the *key* column (or column with identifying information), 3) the *value*
column (the one with the numbers/values). We'll use a pipe so we can ignore the
data argument.

(ref:long-wide) 'Spreading' from long to wide. Taken from the [R for Data Science book](https://r4ds.had.co.nz/tidy-data.html#spreading).

```{r image-long-to-wide, echo=FALSE, fig.cap="(ref:long-wide)"}
# TODO: fix this. Get image directly.
# include_graphics("https://d33wubrfki0l68.cloudfront.net/8350f0dda414629b9d6c354f87acf5c5f722be43/bcb84/images/tidy-8.png")
```

```{r}
# Using a small dataset:
table2

# Convert to wide form
table2 %>% 
    spread(key = type, value = count)
```

The key is the discrete value that will make up the new column names, while the
value will be column that will make up the values of the new columns.

In the function `pivot_wider()` we want to specify three arguments: 

- the dataframe to work on,
- the argument `names_from`: where we indicate the name of the "long" variable that we will use to create the new "wide" variables 
- the argument `values_from`: the values that we will use to fill the rows of the new variable we created

```
population_wide <- population %>% 
  pivot_wider(names_from = country, values_from = population) 
population_wide
```

## Exercise 1: Reshaping a dataframe from long to wide format

Use the dataframe `us_rent_income` for this exercise. This dataframe shows the estimated value and the 90%
margin of error for yearly income and monthly rent of each state in the US.

In this case, you want to display the data in wide form to see how estimated income varies across states.
To do so, each state will become a variable (i.e. column) to which the estimated income will be associated


## Exercise 2: Reshaping a dataframe from wide to long format

We will use again the `us_rent_income` dataframe for this exercise.
In this case we want the values of `estimate` and `moe` to be stacked in one single column called `value`, 
while the respective categories of `estimate` and `moe` should be described in a variable called `calculation`

```{r, eval = FALSE}
us_rent_income_long <- us_rent_income %>% 
  # this variable is a nuisance and is not giving information useful to us, 
  # so we will remove it 
  select(-GEOID) %>% 
  # When we want multiple variables, we combine them with `c()`.
  pivot_longer(c(______, ______), names_to = ________, values_to = _________)
us_rent_income_long
```

### `pivot_**` with `group_by()` and `summarize()`

For this example we are going to use the built-in dataframe `storms`

In this dataframe, information about storms are classified in long format. 
Storms are categorized across years and information on wind, latitude, longitude and pressure 
are presented as columns.

Let's say that we want to see how mean wind speed vary across 
different storm categories (`status`) and across years. 

This means that we want to apply the function `pivot_wider()` to visualize each storm category as a separate variable.

However, before reshaping the dataframe in long form, we need to do some extra work. 
Take another look at the `storms` dataframe. Variables are measured multiple times within each year. 
So if we want to have a mean value of wind speed for each year, we need to use the functions `group_by()` and `summarize()` that we showed in the intro session. 

We are going to call this new dataframe `storm_wide`

First we will calculate the mean value of wind speed for each storm, across years 
and discriminating across storm status

Now we can reshape the data into a wide format:

```
storms_wide <- storms_sum %>% 
  pivot_wider(names_from = status, values_from = mean_wind)  
storms_wide
```

## Exercise 3: using `group_by()` and `summarize()` in combination with `pivot_**()` functions to reshape a data frame

Using the dataframe `diamonds` use the functions `group_by()` and `summarize()` in combination with
`pivot_longer()`, `pivot_wider()` to reshape a database in which different values of cut correspond to mean values of x, y, z (tip: you need to use both pivot_longer and pivot_wider!)

The reshaped dataframe should look like this:


## Final exercise: Group work

Time: ~30 min

This exercise has two aims: to get working on and completing the group project, 
and to get you practicing using the dplyr and tidyr functions we covered today.
First, take maximum 10 min to:

- *As a group*, complete item 2 of the [group assignment](assignment)
(to jump quickly to the assignment, 
run `r3::open_assignment()` in the RStudio Console).
- *Individually*, open your group R project
and complete item 3 of the group assignment.
Follow the appropriate filenaming conventions as we learned in the Project Management
session for the dataset (e.g. don't use spaces, instead use either `-` or `_`).

With the remaining time:

- *As a group*, complete item 4 of the [group assignment](assignment).
Explore the data and *use all the functions* we've covered in this session to
better understand the data you'll work with. 
You can (and probably should) divide exploratory tasks between group members,
so that you have a good understand *as a group* of the data.

> *Tip*: Make use of TAB auto-completion when typing out the dplyr and tidyr
functions for wrangling the data to speed up your coding and to get help if needed.
For instance, type out `sel`, hit TAB, and see the list of possible functions.
Choose the right item in the menu and hit TAB again to finish the function.
If you want to see a list of other functions in dplyr, type out `dplyr::` 
and then hit TAB. You'll now have a list of functions inside the dplyr package.
