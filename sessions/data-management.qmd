# Data management and wrangling {#sec-wrangling}

```{r setup}
#| include: false
library(tidyverse)
```

<!-- TODO: Add an introduction here -->

**Session objectives**:

<!-- TODO: Revise objectives to fit how I did it in advanced -->

1.  Learn the difference between "messy" and "tidy" data, including how
    to create tidy data to simplify your analysis.
2.  Perform simple transformations and subsetting of datasets, such as:
    -   Subset specific columns and rows of a dataset, with
        `filter()`and `select()`.
    -   Sort rows of a dataset by a specific column, with `arrange()`.
    -   Create new or transform existing columns in a dataset, with
        `mutate()`.
    -   Calculate simple data summaries, with `summarise()`.
3.  Learn about and apply the "split-apply-combine" method of analyses,
    with `group_by()` and `summarise()`.
4.  Write "tidier" and more readable code by using the pipe (`%>%`)
    operator.

## "Messy" vs. "tidy" data

::: {.callout-note collapse="true"}
## Instructor note

This session usually takes a fair amount of time relative to the other
sessions. The Quarto session is right afterwards and since it usually is
done quicker than other sessions, it's fine if this session takes up
time in the Quarto session.

For the reading section, emphasize the characteristics of a "tidy"
dataset.
:::

::: callout-note
## Reading task: \~10 minutes

The concept of "tidy" data was popularized in an
[article](https://www.jstatsoft.org/v59/i10/paper) by Hadley Wickham and
described in more detail in the [Tidy Data
chapter](https://r4ds.had.co.nz/tidy-data.html) of the *R for Data
Science* online book. Before we continue with tidy data, we need to
cover something that is related to the concept of "tidy" and that has
already come up in this course: the `{tidyverse}`. The `{tidyverse}` is
an ecosystem of R packages that are designed to work well together, that
all follow a strong "[design philosophy](https://design.tidyverse.org/)"
and common [style guide](https://style.tidyverse.org/). This makes
combining these packages in the tidyverse much easier. We teach the
tidyverse because of these above mentioned reasons.

Ok, back to "tidy data". A tidy dataset is when:

-   Each variable has its own column (e.g. "Body Weight").
-   Each observation has its own row (e.g. "Person").
-   Each value has its own cell (e.g. "Body weight for a person at a
    specific date").

Take a look at the example "tidy" and "messy" data frames (also called
"tibbles" in the tidyverse) below. Think about why each may be
considered "tidy" or "messy". What do you notice between the tidy
versions and the messier versions?

```{r}
# Datasets come from tidyr
# Tidy:
table1

# Partly tidy:
table2

# Messier:
table3

# Messy:
table4a

# Messy:
table4b
```

The "most" tidy version is `table1` as each column describes their
values (e.g. population is population size), each row is unique (e.g.
first row is for values from Afghanistan from 1999), and each cell is an
explicit value representative of its column and row.

`table2` is a "long" version of `table1` so it is partly "tidy", but it
doesn't satisfy the rule that each variable has a column, since `count`
represents both cases and population size.

On the other hand, `table3` is messy because the `rate` column values
are a composite of two other column values (cases and population), when
it should be a single number (a percent). Both `table4a` and `table4b`
have columns with ambiguous values inside. For example, you can't tell
from the data what the values in the `1999` column contain.

Tidy data has a few notable benefits:

1.  Time spent preparing your data to be tidy from the beginning can
    save days of added work and frustration in the long run.
2.  "Tidy data" is a conceptual framework that allows you to easily
    build off and wrangle (i.e. "manipulate", "clean up", "manage") data
    in simpler and easy-to-interpret ways, especially when used within
    the framework of the tidyverse.

The concept of tidy data also gives rise to "tidy code" for wrangling.
By using "verbs" (R functions) and chaining them together in "sentences"
(in a sequential pipeline), you can construct meaningful and readable
code that describes in plainer English what you are doing to the data.
This is one simple way that you can enhance the reproducibility of your
code.
:::

## Managing and working with data in R

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

After they read it, go over and emphasize the functions shown in the
table that we will be teaching.
:::

::: callout-note
## Reading task: \~5 minutes

When working with data, there are a few principles to follow:

-   You should always save your original raw dataset in the `data-raw/`
    folder.
    -   *Note*: Whether or not you save data to `data-raw/` depends on
        how you collected the data and how many collaborators are on
        your team. You may end up storing and processing the data in
        another folder as a project of its own.
-   **Never** edit your raw data directly and save it in a separate
    location.
-   Only work with your raw data using R code. *Don't manually edit it*.
    Manual editing doesn't leave a history of what you've done to it, so
    you can't go back and see what you've done. Always keep a history of
    any changes you've made to the data, preferably by using R code.
-   Save the edited data as another dataset and store it in the `data/`
    folder.

When wrangling your data with R code, make sure to:

-   Document what you did to your data and why you did it, to help you
    remember later on (by using hashes to indicate comments and non-code
    text).
-   Write the code in a way that is as descriptive as possible and is
    readable enough to understand what is being done to the data.
-   Keep the code simple: Don't be clever, be clear. Clear code is
    easier to understand than clever and sometimes overly-complex code.

In data wrangling, most tasks can be expressed by a few simple "verbs"
(actions). Wrangling here refers to maneuvering, managing, controlling,
and turning your data around to clean it up, better understand it, and
prepare it for later analyses. The table below lists some common "verbs"
from the `{dplyr}` package that come from the `{tidyverse}`:

| Task                        | Example                                                                   | Function                        |
|:----------------------------|:--------------------------------------------------------------------------|:--------------------------------|
| Select columns              | Remove data entry columns such as the person's name who entered the data. | `select()`                      |
| Rename columns              | Changing a column name from 'Q1' to 'participant_name'.                   | `rename()`                      |
| Transform or modify columns | Multiplying or taking the log of a column's values.                       | `mutate()`                      |
| Subset/filter rows          | Keeping rows with glucose values above 4.                                 | `filter()`                      |
| Sort rows                   | Show rows with the smallest value at the top.                             | `arrange()`                     |
| Calculate summaries         | Calculating the maximum, median, and minimum age.                         | `summarise()`                   |
| Run code by a group         | Calculate means of age by males and females.                              | `group_by()` with `summarise()` |

: List of common data wrangling tasks, along with an example and the
function used for the wrangling.

**Tip**: Sometimes you need to do some complicated wrangling to get your
data "in shape" for later analyses. To save some time, you could save
the wrangled data as an "output" dataset in the `data/` folder. That
way, you can easily use it again later rather than having to run the
wrangling code every time you want to work with the data.
:::

## Load the packages and dataset

We're going to use the US
[NHANES](https://www.cdc.gov/nchs/nhanes/index.htm) dataset to
demonstrate some wrangling functions. There is a R package called
`{NHANES}` that contains a teaching version of the original dataset, so
we'll use that for this course.

First, make sure that you have the `LearningR` R Project open. Second,
open the `R/learning.R` script and write code to load the `{NHANES}`
package. It should look like this now:

```{r add-packages-to-script}
#| filename: "R/learning.R"
library(tidyverse)
library(NHANES)
```

There are many ways of loading in data into R. This method loads data
that is included in a package, in this case `{NHANES}`. We will cover
how to load a dataset from a file at the end of this session.

Then, below the `library()`, let's take a `glimpse()` at the NHANES
dataset. Run this function by hitting `Alt-Enter` while the cursor is on
the code to send it from the script to the R Console.

```{r glimpse-nhanes}
#| filename: "R/learning.R"
# Briefly glimpse contents of dataset
glimpse(NHANES)
```

This gives us a quick overview of what variables are in the dataset and
the data types that they are.

## Selecting specific columns in a dataset

Selecting columns of a dataset is a very common data wrangling task. The
function for this task in RStudio is appropriately called `select()`.
You would use `select()` to extract one or more variables in a dataset
that you want to have a closer look at or to save as a new data frame to
work with. It may be that you wish to explore the clinical
characteristics of your study sample, so you may select some basic
demographic variables (e.g., the `Age` column) and clinical variables
(e.g., `Weight` and `Height` columns) to perform these analyses.

For the input arguments, `select()` takes the dataset as the first
argument, which is the first input position right after the opening
bracket `(`, and then takes the names of the columns you want to select.
The argument after the data argument is `...`, which indicates that you
can add as many columns as you want, separated by a `,`.

```{r using-select}
#| filename: "R/learning.R"
# Select one column by its name, without quotes
select(NHANES, Age)

# Select two or more columns by name, without quotes
select(NHANES, Age, Weight, BMI)

# To *exclude* a column, use minus (-)
select(NHANES, -HeadCirc)
```

If some of your columns have similar patterns for naming at the
beginning, middle, or end of the name, you can use some helper functions
to choose these columns. Use `?select_helpers` (choose the "Select
helpers" option in the menu that pops up) to read more about these
functions and to get help on them. Some commonly used helpers are:

-   `starts_with()`: Select columns that begin with a pattern.
-   `ends_with()`: Select columns that end with a pattern.
-   `contains()`: Select columns that contain a pattern.

```{r using-tidyselect-helpers}
#| filename: "R/learning.R"
# All columns starting with letters "BP" (blood pressure)
select(NHANES, starts_with("BP"))

# All columns ending in letters "Day"
select(NHANES, ends_with("Day"))

# All columns containing letters "Age"
select(NHANES, contains("Age"))
```

For more information on using the pattern functions such as
`starts_with()`, check out `?select_helpers`.

You'll notice that running these functions doesn't actually change the
data itself. When you run a function without assigning it using `<-`,
the only action the function does is to send the output to your screen,
and you won't have saved that data anywhere for later use. But if you
want to create a new dataset with only the columns you selected, you'll
need to assign the selected dataset to a new object.

The full NHANES dataset is 10,000 individuals (rows) with 76 parameters
(columns). To make it easier to look at and since we are only interested
in some of these parameters, we will subset the large dataset into
something smaller and save it for later use as a new dataset.

```{r create-smaller-nhanes}
#| filename: "R/learning.R"
# Save the selected columns as a new data frame
# Recall the style guide for naming objects
nhanes_small <- select(NHANES, Age, Gender, BMI, Diabetes,
                       PhysActive, BPSysAve, BPDiaAve, Education)

# View the new data frame
nhanes_small
```

## Renaming all column names based on the style guide

In the interests of keeping data tidy and matching the [style
guide](https://style.tidyverse.org/), we should change the column names
to be all lower case with `_` for spaces between words. There's a
package that can do that for us called `{snakecase}`.

To change all the column names to snakecase, we'll use the function
`rename_with()`. This function takes the data as the first argument but
the second argument needs to be a function, which in our case is called
`snakecase::to_snake_case()`, but exclude the `()` at the end. This
function will rename all columns.

```{r create-nhanes-snakecase}
#| filename: "R/learning.R"
# Rename all columns to snake case
nhanes_small <- rename_with(nhanes_small, snakecase::to_snake_case)

# Have a look at the data frame
nhanes_small
```

## Renaming specific columns

Depending on how your data was collected, it may have column names that
aren't very descriptive. So you'll probably want to rename them to
something more explanatory, which is particularly important if you're
sharing your work with others or in an environment where multiple people
are working on the same data. We will use the function called
`rename()`. Like `select()`, `rename()` takes the dataset as the first
argument and then takes as many renaming arguments as you want (because
the second argument position is `...`). When renaming, it takes the form
of `newname = oldname`.

The "gender" variable in the dataset actually describes "sex", so let's
rename it to accurately reflect the data itself. Note that gender is a
*social construct*, while sex is *biological*. This unfortunately is
often incorrectly named in many scientific papers, where accuracy of
terms is critical. Because we want to rename the variable in the dataset
so it stays renamed, we need to assign it again to `nhanes_small`.

```{r rename-to-gender}
#| filename: "R/learning.R"
nhanes_small <- rename(nhanes_small, sex = gender)
nhanes_small
```

::: {.callout-tip appearance="default"}
We've re-assigned the object `nhanes_small` multiple times. If you make
a mistake, the easiest solution is to re-run the code from the start
again, which should fix any issues.
:::

Now, you can see that the column has been renamed from `gender` to
`sex`. What if you wanted to select some columns and then rename some of
them? Would you have to create a new data object every time? No! We can
make use of a very powerful tool called piping with the `%>%` function.

## Chaining functions with the pipe

::: callout-note
## Reading task: \~5 minutes

A key component of tidy data and tidy code is making use of the "pipe"
operator, `%>%`. There is also the base R pipe `|>`, which works
basically the same but with some small differences. For this course we
will use `%>%`, though you can use either.

You would use the "pipe" operator when you are writing a piece of code
with multiple operations or intermediate steps that require you to save
and overwrite each step as an object (see below). One advantage of the
"pipe" operator is that it will help to ensure that your code is less
cluttered with redundant and temporary object names.

This operator allows you to "pipe" the output from one function to the
input of another function, just like a plumbing pipe would do for water.
This allows you to easily chain functions together into "sentences".
Let's use an example based on English words for some action. This is the
English sentence:

> We need some eggs. Drive to the grocery store and buy some eggs before
> arriving home from work.

There are basically two actions here ("drive" and "buy") with four
inputs ("work", "grocery store", "eggs", "home"), that are all based on
the previous action. Since an action in R is a function, the functions
would be `drive()` and `buy()`. In regular R (without the "pipe"
operator), we would have to nest functions (reading from the inside to
the outside) together to chain them:

``` r
drive(buy(drive(at_work, "grocery store"), "eggs"), "home")
```

This is difficult to read. Another way to chain functions would be to
create temporary objects for each step:

``` r
at_grocery_store <- drive(at_work, "grocery store")
got_eggs <- buy(at_grocery_store, "eggs")
at_home <- drive(got_eggs, "home")
```

This still isn't very "readable", as we are having to re-name each
intermediate object with reference to the object before it. The pipe
`%>%` operator can really simplify this:

``` r
at_work %>% 
    drive("grocery store") %>% 
    buy("eggs") %>% 
    drive("home")
```

Do you find this more readable and understandable? It reads how it would
be done, in order of the steps taken.

The idea of piping is to read the functions from left to right. This can
help clarify and break down complex data processing workflows, and is
the basis for the tidyverse and many other packages.

The operator `%>%` takes the output from the object or function from the
left of the operator and puts it into the function on the right of the
operator. All input goes into the first position argument of the
function. Within the tidyverse packages, all functions take a data frame
(or vector) as the first argument in order to work with the pipe.
:::

Let's try this out on NHANES. The keyboard shortcut for the pipe is
{{< var keybind.pipe >}} (i.e., M for the `{magrittr}` package that
created the pipe).

```{r show-pipe-with-colnames}
#| filename: "R/learning.R"
# These two ways are the same
colnames(nhanes_small)

nhanes_small %>% 
    colnames()
```

The pipe automatically takes `nhanes_small` and puts it into the first
position, so we don't need to specify `nhanes_small` inside `colnames()`
when piping.

Let's try using the pipe with the `select()` and `rename()` functions
from the previous section. Remember, both `select()` and `rename()` take
a dataset as the first argument, which makes them pipe-able.

```{r piping-select-to-rename}
#| filename: "R/learning.R"
nhanes_small %>% 
    select(phys_active) %>% 
    rename(physically_active = phys_active)
```

We can now "read" these actions as:

> Take the nhanes_small dataset *and then* select the "phys_active"
> column *and then* rename the "phys_active" column to
> "physically_active".

## Exercise: Practice what we've learned

> Time: 10 minutes.

In the `R/learning.R` script file, complete the following tasks, running
each code after completing the task.

1.  Copy and paste the code below into the script file. Replace the
    `___` in the `select()` function, with the columns `bp_sys_ave`, and
    `education`.

    ```{r}
    #| filename: "R/learning.R"
    #| eval: false
    nhanes_small %>% 
        select(___)
    ```

2.  Copy and paste the code below and fill out the blanks. Rename the
    `bp_` variables so they don't end in `_ave`, so they look like
    `bp_sys` and `bp_dia`. *Tip*: Recall that renaming is in the form
    `new = old`.

    ```{r}
    #| filename: "R/learning.R"
    #| eval: false
    nhanes_small %>% 
        rename(___ = ___,
               ___ = ___)
    ```

3.  Re-write this piece of code using the "pipe" operator:

    ```{r}
    #| filename: "R/learning.R"
    #| eval: false
    select(nhanes_small, bmi, contains("age"))
    ```

4.  Read through (in your head) the code below. How intuitive is it to
    read? Now, re-write this code so that you don't need to create the
    temporary `blood_pressure` object by using the pipe, then re-read
    the revised version. Which do you feel is easier to "read"?

    ```{r}
    #| filename: "R/learning.R"
    #| eval: false
    blood_pressure <- select(nhanes_small, starts_with("bp_"))
    rename(blood_pressure, bp_systolic = bp_sys_ave)
    ```

5.  Run `{styler}` on the `R/learning.R` file with
    {{< var keybind.styler >}}.

6.  Lastly, add and commit these changes to the Git history with the
    RStudio Git interface using {{< var keybind.git >}}.

```{r solution-practice-dplyr}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
# 1. Select specific columns
nhanes_small %>%
    select(bp_sys_ave, education)

# 2. Rename columns
nhanes_small %>%
    rename(bp_sys = bp_sys_ave,
           bp_dia = bp_dia_ave)

# 3. Re-write with pipe
nhanes_small %>% 
    select(bmi, contains("age"))

# 4. Re-write with pipe
nhanes_small %>% 
    select(starts_with("bp_")) %>% 
    rename(bp_systolic = bp_sys_ave)
```

## Filtering data by row

Filtering data by row is a very common activity in data analysis,
especially if you want to get rid of outliers or to subset by a
categorical group. The function to subset or filter is called
`filter()`. `filter()` is distinct from `select()` in the sense that it
operates on rows, whereas `select()` operates on columns.

The `filter()` function takes a logic condition (`TRUE` or `FALSE`). As
with the other functions, the first argument is the dataset and all
others are the logical conditions that will apply to the row filtering.
When the logical conditions equal `TRUE`, it means that those rows will
be **kept** and those that are `FALSE` will be *dropped*.

*Warning*: Since `filter()` uses logical conditions, you need to be
really careful when writing the logic. As you probably know, humans are
really *really* bad at logic. If your logical condition starts getting
complex, double and triple check that you know for certain that your
logic code is doing what you think it is doing. It's very easy to make
mistakes at this stage, even for advanced R users.

The simplest kind of logic condition is to test for "equality". In R,
"equal to" is represented by `==`. For example, if we wanted to keep
only people who are not physically active in the dataset, we would use
the logic condition like this:

```{r}
#| filename: "R/learning.R"
#| eval: false
nhanes_small %>%
    filter(phys_active == "No")
```

We would "read" this code as:

> Take the nhanes_small dataset *and then* filter so that only rows
> where `phys_active` is equal to "No" are kept.

When a row in the `phys_active` column has the value `"No"`, that row is
kept. Otherwise, it is dropped.

There are other logic comparisons to use. @tbl-logic-operators can be
used as a reference for logical conditions in R.

```{r logic-operators, echo=FALSE}
#| label: tbl-logic-operators
#| tbl-cap: "Logical operators in R."
tibble::tribble(
    ~Operator, ~ Description,
    "<", "less than",
    "<=", "less than or equal to",
    ">", "greater than",
    ">=", "greater than or equal to",
    "==", "equal to",
    "!=", "not equal to",
    "!x", "Not x (if x is true or false)",
    "x | y", "x OR y",
    "x & y", "x AND y"
) %>% 
    knitr::kable()
```

Let's try out a few of these logical conditions with `filter()`.

```{r}
#| filename: "R/learning.R"
# Participants who are physically active
nhanes_small %>%
    filter(phys_active != "No")

# Participants who have BMI equal to 25
nhanes_small %>%
    filter(bmi == 25)

# Participants who have BMI equal to or more than 25
nhanes_small %>%
    filter(bmi >= 25)
```

We use the `|` ("or") and `&` ("and") when we want to combine conditions
across columns. Be careful with these operators and when combining logic
conditions, as they can sometimes work differently than our human brains
interpret them (speaking from experience). For `&`, both sides must be
`TRUE` in order for the combination to be `TRUE`. For `|`, only one side
needs to be `TRUE` in order for the combination to be `TRUE`. To see how
they work try these:

```{r}
TRUE & TRUE
TRUE & FALSE
FALSE & FALSE
TRUE | TRUE
TRUE | FALSE
FALSE | FALSE
```

When used in `filter()`, combinations of logic conditions may look like
this:

```{r}
#| filename: "R/learning.R"
# When BMI is 25 AND phys_active is No
nhanes_small %>%
    filter(bmi == 25 & phys_active == "No")

# When BMI is 25 OR phys_active is No
nhanes_small %>%
    filter(bmi == 25 | phys_active == "No")
```

## Arranging the rows of your data by column

You may want to sort your rows by a specific column so that values are
arranged in ascending or descending order. This can be done using the
function called `arrange()`. Again, `arrange()` takes the dataset as the
first argument, followed by the columns that you wish to arrange data
by. By default, `arrange()` orders in *ascending* order.

```{r}
#| filename: "R/learning.R"
# Arranging data by age in ascending order
nhanes_small %>%
    arrange(age)
```

`arrange()` also arranges parameters of type `character` alphabetically:

```{r}
#| filename: "R/learning.R"
nhanes_small %>% 
    arrange(education)
```

If we want to order the column based on descending order, this can be
done with `desc()`.

```{r}
#| filename: "R/learning.R"
# Arranging data by age in descending order
nhanes_small %>%
    arrange(desc(age))
```

You can also order your data by multiple columns. For instance, we could
arrange first by `education` and then by `age`.

```{r}
#| filename: "R/learning.R"
# Arranging data by education then age in ascending order
nhanes_small %>%
    arrange(education, age)
```

## Transform or add columns

To modify an existing column or to add a new one, we can use the
function called `mutate()`. You can use `mutate()` to compute a new
variable using existing columns in your dataset. You can multiply all
values in a certain column by 2, or combine columns into a new variable.
Like the other functions, the first input is the dataset and the other
arguments are columns to add or modify.

Take this example: In some analyses, sometimes age is reflected as
months, not years. Since NHANES has age in years, to convert to months
we would multiply by 12. So, we would use `mutate()` with the following
instruction:

```         
age = age * 12
```

This form is similar to how math works. The action that happens on the
right hand side is put into the variable of the left hand side. When
using `mutate()`, it looks like this:

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    mutate(age = age * 12)
```

Like with `filter()`, you can continue to add or modify more columns by
using `,`. So let's do that to a new column (e.g., log transforming
BMI):

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    mutate(age = age * 12,
           logged_bmi = log(bmi))
```

We can also have different values based on logic conditions using
`if_else()`. Use @tbl-logic-operators to help with creating the logic
condition.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    mutate(old = if_else(age >= 30, "Yes", "No"))
```

::: {.callout-tip appearance="default"}
Recall that the original dataset doesn't change. If we want the added
variable to be saved, we must assign it to something with `<-`. Putting
it all together, you would enter something like this (no need to type
this out yourself):

```{r}
#| filename: "R/learning.R"
nhanes_update <- nhanes_small %>%
    mutate(old = if_else(age >= 30, "Yes", "No"))
```
:::

Before moving on, run `{styler}` with {{< var keybind.styler >}} and
commit all the files changes to the Git history with the RStudio Git
interface using {{< var keybind.git >}}.

## Exercise: Piping, filtering, and mutating

> Time: 20 minutes.

Copy and paste the code below into the `learning.R` script file.

```{r ex-pipe-filter-mutate}
#| eval: false
#| filename: "R/learning.R"
# 1. BMI between 20 and 40 with diabetes
nhanes_small %>%
    # Format should follow: variable >= number or character
    filter(___ >= ___ & ___ <= ___ & ___ == ___)

# Pipe the data into mutate function and:
nhanes_modified <- nhanes_small %>% # Specifying dataset
    mutate(
        # 2. Calculate mean arterial pressure
        ___ = ___,
        # 3. Create young_child variable using a condition
        ___ = if_else(___, "Yes", "No")
    )

nhanes_modified
```

Then, start replacing the `___` with the appropriate code to complete
the tasks below. (*Hint*: Create a new "Section" in the R script for
this exercise by using {{< var keybind.code-section >}}).

1.  Filter `nhanes_small` so only those participants with a BMI of more
    than or equal to 20 *and* less than or equal to 40, *and* keep those
    who have diabetes.

2.  Create a new variable called `mean_arterial_pressure` by applying
    the formula:

    $$((2 * DBP) + SBP) / 3$$

    (DBP = `bp_dia_ave` and SBP = `bp_sys_ave`) to calculate [Mean
    Arterial
    Pressure](https://en.wikipedia.org/wiki/Mean_arterial_pressure).
    *Hint*: In R, use `+` to add, `*` to multiply, and `/` to divide.

3.  Create a new variable called `young_child` for cases where age is
    less than 6 years.

4.  Finally, add and commit these changes to the Git history with the
    RStudio Git Interface. Push to GitHub to synchronize with your
    GitHub repository.

```{r solution-pipe-filter-mutate}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
# 1. BMI between 20 and 40, with diabetes
nhanes_small %>%
    filter(bmi >= 20 & bmi <= 40 & diabetes == "Yes")

# Pipe the data into mutate function and:
nhanes_modified <- nhanes_small %>% # dataset
    mutate(
        mean_arterial_pressure = ((2 * bp_dia_ave) + bp_sys_ave) / 3,
        young_child = if_else(age < 6, "Yes", "No")
    )

nhanes_modified
```

## Split-apply-combine: Summarizing data

::: callout-note
## Reading task: \~5 minutes

Summarising or applying simple (or complex) statistics to data is a key
component of any analysis. Simple summaries or statistics can be done
either on all of the data or on groups of it. There are many data
analysis tasks that can be approached using the
[split-apply-combine](https://www.jstatsoft.org/article/view/v040i01)
method, which involves splitting the data into groups, applying some
analysis to each group, and then combining the results together.

In `{dplyr}`, you can use the function called `summarise()` to summarise
on all the data. If you want to do a split-apply-combine analysis to,
e.g., find the max height by education status, you would use the
functions `group_by()` and then `summarise()`. Using `group_by()` splits
the data up and `summarise()` applies an analysis, then immediately
combines it back together.

The first position argument to `group_by()` is, as usual, the dataset.
The next arguments are the columns that contain the values you want to
group by. These columns must contain **categorical** data (e.g.
education). `group_by()` tells R to compute the next operations on the
data within each grouping, rather than on all the data.

As with the other functions, `summarise()` takes the dataset as the
first position argument. The next arguments work similar to the
arguments in `mutate()` with one difference: the output must create a
single value (e.g. a mean). Like `mutate()`, you can add multiple
"summaries" by adding new columns separated by `,`. You would use
`summarise()` to derive basic descriptive statistics of a certain
variable, including `min()`, `max()`, `mean()`, `median()`, or `sd()`
(standard deviation).

The `group_by()` function doesn't do anything by itself so should always
be used in combination with a `summarise()`, `mutate()`, `arrange()`, or
other function. However, the `summarise()` function can be used on its
own.
:::

## Calculating summary statistics

Let's calculate the maximum value of the BMI variable. See what happens
when you enter the following:

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    summarise(max_bmi = max(bmi))
```

We get back a result of `NA`, which means "missing". In R, `NA` values
"propagate", meaning that if there is one value missing, then the max or
mean will also be missing. So, we need to tell `max()` to exclude any
`NA` values from the calculation using the argument `na.rm = TRUE`.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    summarise(max_bmi = max(bmi, na.rm = TRUE))
```

To calculate another summary statistic, you would add another summary
column using `,`:

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    summarise(max_bmi = max(bmi, na.rm = TRUE),
              min_bmi = min(bmi, na.rm = TRUE))
```

Before you start the following exercise, add and commit changes to the
Git history with the RStudio Git interface.

## Summary statistics by a group {#sec-group-by-summarise}

While the `summarise()` function is useful enough on its own, it really
shines when combined with `group_by()`.

Let's use these functions to find out the mean age and BMI between those
with and without diabetes.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    group_by(diabetes) %>% 
    summarise(mean_age = mean(age, na.rm = TRUE),
              mean_bmi = mean(bmi, na.rm = TRUE))
```

*Quick note*: If you are using a version of dplyr \>= 1.0.0, you'll get
a message informing you that it is `regrouping output`. This is a
notification and can be ignored. If you don't want the message
displayed, you can add `options(dplyr.summarise.inform = FALSE)` to the
top of your script and run it.

We also get a warning about there being missing values in diabetes, so
we need to remove rows that have missing diabetes status values.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    # Recall ! means "NOT", so !is.na means "is not missing"
    filter(!is.na(diabetes)) %>% 
    group_by(diabetes) %>% 
    summarise(mean_age = mean(age, na.rm = TRUE),
              mean_bmi = mean(bmi, na.rm = TRUE))
```

Cool! We can add more columns to the grouping, so let's do that. Let's
compare mean age and BMI by physical activity and diabetes status.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    filter(!is.na(diabetes)) %>% 
    group_by(diabetes, phys_active) %>% 
    summarise(mean_age = mean(age, na.rm = TRUE),
              mean_bmi = mean(bmi, na.rm = TRUE))
```

Since we don't need the dataset grouped anymore, it's good practice to
end the grouping with `ungroup()`.

```{r}
#| filename: "R/learning.R"
nhanes_small %>%
    filter(!is.na(diabetes)) %>% 
    group_by(diabetes, phys_active) %>% 
    summarise(mean_age = mean(age, na.rm = TRUE),
              mean_bmi = mean(bmi, na.rm = TRUE)) %>% 
    ungroup()
```

Before you start the following exercise, run `{styler}` using
{{< var keybind.styler >}} and commit changes to the Git history using
{{< var keybind.git >}}.

## Saving datasets as files

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Go over how to save data to a file, but don't cover the `{here}` package
right away. Let them read about it in the section below and then briefly
cover it again.
:::

The `nhanes_small` data frame you created is only available after you've
created it from NHAHES, but if you want to access it later, you can save
it as a `.csv` file in your `data/` folder using the function
`readr::write_csv()`.

```{r save-data-as-csv}
#| filename: "R/learning.R"
readr::write_csv(
  nhanes_small,
  here::here("data/nhanes_small.csv")
)
```

::: callout-note
## Reading task: \~5 minutes

There's a new thing `here()`! The `{here}` package uses a function
called `here()` to make it easier to manage *file paths* within an R
Project.

So, what is a file path and why is this `{here}` package necessary? A
file path is the list of folders a file is found in. For instance, your
CV may be found in `/Users/Documents/personal_things/CV.docx`. The
problem with file paths when running code (like with R) is that when you
run a script interactively (e.g. what we do in class and normally), the
file path and "working directory" (the R Session) are located at the
Project level (where the `.Rproj` file is found). You can see the
working directory by looking at the top of the RStudio Console.

But! When you `source()` an R script or run it *not* interactively, the
R code may likely run *in the folder it is saved in*, e.g. in the `R/`
folder. So your file path `data/nhanes_small.csv` won't work because
there isn't a folder called `data/` in the `R/` folder.

```         
LearningR <-- R Project working directory starts here.
├── R
│   ├── README.md
│   └── learning.R <-- Working directory when running not interactively.
├── data
│   └── README.md
├── data-raw
│   └── README.md
├── doc
│   └── README.md
├── .gitignore
├── DESCRIPTION
├── LearningR.Rproj <-- here() moves file path to start in this file's folder.
├── README.md
└── TODO.md
```

Often people use the function `setwd()` in scripts, but this is *never*
a good idea since using it makes your script *runnable only on your
computer*... which makes it no longer reproducible. We use the `here()`
function to tell R to go to the project root (where the `.Rproj` file is
found) and then use that file path. This simple function can make your
work more reproducible and easier for you to use later on.
:::

For many projects, it isn't necessary or advisable to save every single
data object you create. It's better to let the code create the data
you'll use rather than saving each new wrangled dataset you might
create. However, sometimes you will want or need to save the dataset
you've been working on, perhaps because you've done a lot of work to
prepare it for later analyses, or because you've run an analysis and
want to save the results. In these cases, you should definitely save the
new cleaned dataset.

## Loading in a dataset

::: callout-note
## Reading task: \~5 minutes

We've been using a teaching dataset that we load from a package, mainly
so that we can focus on getting familiar with data wrangling. However,
there will come a time when you want to wrangle your own data. There are
several ways to load in a dataset, with the most common being:

1.  Using the RStudio menu
    `File -> Import Dataset -> From Text/Excel/SPSS/SAS/Stata`
    (depending on your file type you want to import).

2.  If the file is a `.csv` file, use `readr::read_csv()` to import the
    dataset, for instance with the `nhanes_small`:

    ``` r
    nhanes_small <- readr::read_csv(here::here("data/nhanes_small.csv"))
    ```

3.  If the dataset is a `.rda` file, use `load()`:

    ``` r
    load(here::here("data/dataset_name.rda"))    
    ```

    This loads the dataset into your R session so that you can use it
    again.

For SAS, SPSS, or Stata files, you can use the package
[haven](https://haven.tidyverse.org/) to import those types of data
files into R.
:::

## Summary

-   With tidy data, each variable has its own column, each observation
    has its own row, and each value has its own cell.
-   Use the tidyverse to load in multiple packages to tidy up data.
-   Never edit raw data. Instead, use R code to make changes and clean
    up the raw data, rather than manually editing the dataset.
-   Use the functions `select()`, `rename()`, `filter()`, `mutate()`
    ("change or modify"), `arrange()` and `summarise()` from the dplyr
    package to wrangle your data.
-   Use the pipe (`%>%`) to write easy-to-read code, similar to reading
    a text consisting of multiple sentences.

